{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8914e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkConf = org.apache.spark.SparkConf@4d322957\n",
       "spark = org.apache.spark.sql.SparkSession@6b02397b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6b02397b"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val sparkConf = new SparkConf()\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "            .config(sparkConf)\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8ee9ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customers = [customer_id: int, customer_fname: string ... 7 more fields]\n",
       "orders = [order_id: int, order_date: timestamp ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: timestamp ... 2 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customers = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"header\", true)\n",
    "                .option(\"inferSchema\", true)\n",
    "                .option(\"path\", \"/user/itv004483/sparkinput/customers.csv\")\n",
    "                .load()\n",
    "\n",
    "val orders = spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"header\", true)\n",
    "                .option(\"inferSchema\", true)\n",
    "                .option(\"path\", \"/user/itv004483/sparkinput/orders.csv\")\n",
    "                .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84b7f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|countOfOrders|\n",
      "+-----------+-------------+\n",
      "|          3|            7|\n",
      "|          4|            6|\n",
      "|          7|            8|\n",
      "|          8|            8|\n",
      "|          9|            6|\n",
      "|         12|            8|\n",
      "|         13|            6|\n",
      "|         14|            9|\n",
      "|         16|            7|\n",
      "|         18|            8|\n",
      "|         19|            9|\n",
      "|         20|            7|\n",
      "|         22|            6|\n",
      "|         23|            8|\n",
      "|         27|            7|\n",
      "|         31|            6|\n",
      "|         34|            7|\n",
      "|         38|            6|\n",
      "|         40|            7|\n",
      "|         41|            6|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "condition = (order_customer_id = customer_id)\n",
       "joinDF = [customer_id: int, countOfOrders: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[customer_id: int, countOfOrders: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//find number of orders by each customer\n",
    "import org.apache.spark.sql.functions.{col, sum, count}\n",
    "\n",
    "val condition = orders.col(\"order_customer_id\") === customers.col(\"customer_id\")\n",
    "\n",
    "val joinDF = orders.join(customers, condition, \"inner\")\n",
    "                    .select(\"customer_id\", \"order_id\")\n",
    "                    .groupBy(\"customer_id\")\n",
    "                    .agg(count(\"order_id\").alias(\"countOfOrders\"))\n",
    "                    .filter(col(\"countOfOrders\") > 5)\n",
    "                    .sort(\"customer_id\")\n",
    "                \n",
    "\n",
    "joinDF.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "737ff157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|CountOfOrders|\n",
      "+-----------+-------------+\n",
      "|          3|            7|\n",
      "|          4|            6|\n",
      "|          7|            8|\n",
      "|          8|            8|\n",
      "|          9|            6|\n",
      "|         12|            8|\n",
      "|         13|            6|\n",
      "|         14|            9|\n",
      "|         16|            7|\n",
      "|         18|            8|\n",
      "|         19|            9|\n",
      "|         20|            7|\n",
      "|         22|            6|\n",
      "|         23|            8|\n",
      "|         27|            7|\n",
      "|         31|            6|\n",
      "|         34|            7|\n",
      "|         38|            6|\n",
      "|         40|            7|\n",
      "|         41|            6|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "//spark sql\n",
    "\n",
    "customers.createOrReplaceTempView(\"customers\")\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select customer_id, count(order_id) as CountOfOrders \n",
    "    from customers c\n",
    "    join orders o\n",
    "        on c.customer_id = o.order_customer_id\n",
    "    group by customer_id\n",
    "    having CountOfOrders > 5\n",
    "    order by customer_id\n",
    "\"\"\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "453bdce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders limit 5\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fbda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
